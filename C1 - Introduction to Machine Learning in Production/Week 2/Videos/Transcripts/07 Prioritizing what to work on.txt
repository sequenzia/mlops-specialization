In the last video, you learned
about brainstorming and tagging your data with
different attributes. Let's see how you
can use this to prioritize where to
focus your attention. Here's the example we
had previously with four tags and the accuracy
of the algorithm, human level performance
and what's the gap between the current accuracy and
human level performance. Rather than deciding to work on car noise because the
gap to HLP is bigger, one other useful
factor to look at is what's the percentage
of data with that tag? Let's say that 60 percent of
your data is clean speech, four percent is data
with car noise, 30 percent has people noise, and six percent is
low bandwidth audio. This tells us that if we
could take clean speech and raise our accuracy from 94-95 percent on all
the clean speech, then multiplying
one percent with 60 percent just tells us that, if we can improve our
performance on clean speech, the human level performance, our overall speech
system would be 0.6 percent more accurate, because we would do one percent better on 60 percent
of the data. This will raise average
accuracy by 0.6 percent. On the car noise, if we can improve the performance by four percent on four percent of the data, multiplying that
out, that gives us a 0.16 percent improvement and
multiply these out as well, we get 0.6 percent and well, this is essentially zero percent because you can't
make that any better. Whereas previously, we
had said there's a lot of room for improvement
in car noise, in this slightly
richer analysis, we see that because people noise accounts for such a large
fraction of the data, it may be more worthwhile to
work on either people noise, or maybe on clean speech because there's actually
larger potential for improvements in both of those than for speech with car noise. To summarize, when
prioritizing what to work on, you might decide on the most important categories
to work on based on, how much room for improvement
there is, such as, compared to human-level
performance or according to some
baseline comparison. How frequently does
that category appear? You can also take
into account how easy it is to improve accuracy
in that category. For example, if you
have some ideas for how to improve the accuracy
of speech with car noise, maybe your data augmentation, that might cause you to
prioritize that category more highly than some other
category where you just don't have as many ideas for
how to improve the system. Then finally, how important it is to improve performance
on that category. For example, you may decide that improving performance with car noise is
especially important because when you're driving, you have a stronger
desire to do search, especially search on maps
and find addresses without needing to use your
hands if your hands are supposed to be holding
the steering wheel. There is no mathematical formula that will tell you
what to work on. But by looking
at these factors, I hope you'd be able to make
more fruitful decisions. Once you've decided that
there's a category, or maybe a few
categories where you want to improve the
average performance, one fruitful approach
is to consider adding data or improving the quality
of that data for that one, or maybe a small
handful of categories. For example, if you
want to improve performance on speech
with car noise, you might go out and collect
more data with car noise. Or if you have a way of using data augmentation to get more
data from data category, that will be another way to improve your algorithm's
performance. One topic that we'll
discuss next week is how to improve label accuracy
or data quality. You'll learn more about
this when we talk about the data phase of the machine
learning project lifecycle. In machine learning, we always would like
to have more data, but going out to collect
more data generically, can be very time-consuming
and expensive. By carrying out an
analysis like this, when you are then going through this iterative process of improving your
learning algorithm, you can be much more focused in exactly what types
of data you collect. Because if you decide to collect more data with car noise
or maybe people noise, you can be much more specific in going out to collect
more of just that data or using data augmentation without wasting time
trying to collect more data from a low bandwidth
cell phone connection. This focus on
improving your data on the tags that you
have determined are most fruitful for
you to work on, that can help you be much more efficient in how you improve your learning
algorithm's performance. I found this type of
error analysis procedure very useful for
many of my projects and I hope it will
help you too in building production-ready
machine learning systems. Next, one of the most
common challenges we run into is skewed datasets. Let's go on to the next
video to go through some techniques for
managing skewed datasets.