Now that you've seen some of the data issues and
detection workflows and got an idea of the importance of productions
scale data validation. Let's take a look at
TensorFlow Data Validation, which is a library from Google as part of
the TFX Ecosystem. It'll allow you to do data validation using Python and we'll do an
exercise doing that. TensorFlow Data
Validation or TFDV, helps developers
understand, validate, and monitor their
ML data at scale. TFDV is used to analyze and validate petabytes
of data at Google every day across
hundreds or thousands of different applications that
are currently in production. TFDV helps TFX users maintain the health of their ML pipelines. TFDV helps generate
data statistics and provides browser
visualizations. It also helps infer the
schema for your data, but you will need to make sure that that
schema makes sense. It'll do an inference
that is best effort. Once it has these
statistics and schema, it can look for problems
are anomalies in your data. Then it will look at the
training serving skew by comparing the data in your training and your
serving datasets. One of the common use cases is continuously checking newly arriving data
by validating it against the expectations
which you have in the reference schema that you generated from
your training data. The typical setup
uses the schema, which is maintained over time, and the statistics are
computed over new data and those statistics are used to validate the data against
the original schema. Remember, we talked about
skew detection and with TFDV you can easily detect
three different types of skew, schema skew, feature skew,
and distributions skew. TFDV performs skew or
drift detection on categorical features and skew is expressed in terms of
an L-infinity distance, which is also known as
Chebyshev distance. If you think of a chessboard, it's the distance metric that is the maximum absolute distance in one-dimension of two
n-dimensional points. You can set thresholds
so that you'll receive warnings when the drift is higher than what you
think is acceptable. Schema skew occurs when the serving and training data don't conform to the same schema. For example, it could
be a change in type, an int, where you're
expecting a float, which could be a change
in the feature itself. Feature skew are changes in the feature values between
training and serving. It could happen as the system
uses different data sources during training and serving or things change or of course, things like seasonality
and trend as well. Sometimes that simply because you have two different code paths and you're trying to do
the same transformations, both when you trained
your model and when you're serving
your model using two different code paths and you're getting different
results as a consequence. There's ways to avoid doing that and we'll talk
about that later. Distribution skew is changes in the distribution of individual
features in the dataset. Features that's in training
might have a range of 0-100 when you're training
it and then at serving time, you're seeing data between 5-600. That would be a change in the distribution
for that feature. You got to have things
like changes is the mean or the median or the
standard deviation changes, all of those are changes
in distribution. Depending on how severe it is, it may or may not be a problem. The question is, does it
affect your model performance enough that you need to make changes to try to account for it? TFDV provides you with
descriptive statistics at scale. Remember, we could be working
with petabytes of data. It provides some
visualizations as well to help you monitor and really
understand that data. Trying to understand the
underlying statistics for your data and do comparisons. How does your training and evaluation and serving
datasets compare? Just in terms of statistics, for example, do they
have the same mean? How can you calculate and fix or detect rather and
fix data anomalies. We did a lot here. Just to wrap up, this week, you saw differences
between ML modeling in academic or research environments and production ML systems. We discussed responsible
data collection and how to really approach building a
fair production ML system. We learned about
process feedback and direct labeling and
also human labeling. We looked at some of the
issues that you can have with data and how to identify
and detect those issues. Now, we're going to be practicing Data
Validation with TFDV, TensorFlow Data Validation in this week's exercise notebook. You'll be testing
your skills with the programming assignment
by generating datasets, statistics, and
creating and comparing and updating data
schemas. Good luck.