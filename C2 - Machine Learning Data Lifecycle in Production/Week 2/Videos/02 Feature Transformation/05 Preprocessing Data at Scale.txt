We've probably all done feature
engineering before, but it's one thing to do it in a notebook
with maybe a few 100 megabytes of data. And quite another thing to do it in
a production environment with maybe a couple of terabytes of data in
a repeatable automated process. Let's take a look now at how to do
feature engineering at scale, okay? So it's one thing to
do transformations and feature engineering on a one
off basis saying a notebook. But it's another thing to do it at
scale in a production environment in a reproducible and
repeatable and consistent way. So let's take a look now at
pre processing data at scale. So first let me tell you a story. I once worked as a data scientist
in a production environment where we were deploying our
workloads are production workloads on Apache Storm and
it it was a nice platform. The performance was was pretty good on it. We as data scientists created
our models in notebooks and then deployed them into Apache Storm. Well it wasn't quite that simple. We were developing our
notebooks in python and then when we deployed into
storm we had to translate all of the feature engineering
that we did into java. Well, as you can imagine,
that was not ideal and there were little weird
problems that cropped up often very difficult to find
doing that transformation. This is not an ideal way to do
production machine learning. So what is much better
technique is to use a pipeline, a unified framework where
you can both train and deploy with consistent and
reproducible results. So we'll be learning about
that in this course. Let's take a look now at
some of the issues of doing feature engineering at scale. So here's what we're going to be talking
about inconsistencies in feature engineering very important. Preprocessing granularity. Also an interesting issue,
working in production environment. Preprocessing the training data set is
one thing and optimizing the instance level transformations for that is,
something kind of different. But there's a number of challenges
that you need to deal with. So to preprocessed data at scale,
we start with real world models and these could be terabytes of data. So when you're doing this
kind of kind of work, you want to start with a subset
of your data work out. As many issues as you can think about
how that's going to scale up to the terabytes that you you need to
actually work with your whole dataset. Large scale data processing
frameworks are no different than what you're going to use, on your laptop or
in a notebook or what have you. So you need to start thinking about
that from the beginning of the process, how this is going to be applied and the earlier you can start
working with those frameworks. The more you work out the issues
early with smaller datasets and quicker turnaround time, consistent
transformations between training and serving are incredibly important. If you do different transformations
when you're serving your model than you did when you were training,
your model, you are going to have problems and often
those problems are very hard to find. So inconsistencies and feature engineering
the training and serving code paths. When you are training your model, you
have code that you're using for training. If you're using different code,
like we were we were using python for training and java for serving,
that's a potential source of problems. So, and you could have
different deployment scenarios. You you might be deploying your model for
in different environments. You you could be using the same model,
say, in a servant cluster and
using it on an IOT device as well. So there's different
deployment targets and you need to think about those and
what are the CPU resources or the compute resources that you
have available on those targets. So mobile, for example,
very tight compute resources, servers, you have more luxury, but
again, cost is a big factor and in a web browser, that could be
deployed on different clients. So you need to think about that as well. You don't want to be too heavy. Though, there's risks in introducing
training, serving skews. So that's what happens because of those
different code paths between training and serving. If you don't have exactly the same
transformation is happening between the two and the best way to do that
is used exactly the same code. Then you have a potential problem and that's going to lower
the performance of your model. So your model may, completely just
error out or give wacky results or it may be just slightly off
in certain circumstances and display sensitivity around certain things. Those things are much harder to detect. So there is a notion of the granularity
at which you're doing preprocessing. So you need to think about this,
you're going to do transformations, both the instance level and
a full pass over your data. And depending on the transformation
that you're doing, you may be doing it in one or the other. You can usually always do instance level. But full path requires that
you have the whole dataset. So for clipping, even for clipping,
you need to set clipping boundaries. If you're not doing that through some
arbitrary setting of those boundaries, if you're using the data set
itself to determine how to clip, then you're going to
need to do a full pass. So min max for
clipping is is going to be important. Doing a multiplication at the instance
level is fine, but scaling well now I'm going to need probably the standard
deviation and maybe the min and max. Bucketizing similarly, unless I know ahead
of time what buckets are going to be, I'm going to need to do a full pass
to find what buckets makes sense. Expanding features I can probably
do that at the instance level. So these two things
are different at training time. I have the whole dataset so
I can do a full pass, although it can be expensive to do that. At serving time,
I get individual examples, so I can only really do instance level. So anything I need to do that requires
characteristics of the whole dataset. I need to have that saved off so
I can use it at serving time. So when do you transform, you're going to
pre-process your training dataset and there's pros and cons in how you do that. First of all, you you don't you only run
once per training session, so that's cool. And you're going to compute
it on the entire dataset, but but only once, for
each training session. The Collins well, you're going to have to
reproduce all those transformations that serving or save off the information
that you learned about the data, like the standard deviation. So that you can use that
later while you're serving. And there's,
slower iterations around this. Each each time you make a change, you've
got to make a full pass over the dataset. So you can do this within the model. Transforming within the model has some
nice features to there are cons as well. First of all it makes iteration somewhat
easier because it's embedded as part of your model and there's guarantees around
the transformation that you're doing. But the cons are it can be
expensive to do those transforms, especially when,
your compute resources are limited. And think about when you do
a transform within the model, you're going to do that same
transform at serving time. So you may have say GPUs or
TPUs when you're training, you may not when you're serving. So there's long model latency,
that's when you're serving your model, if you're doing a lot of
transformation with it that can slow down the response time for
your model and increase latency. And, you can have transformations per
batch that that skew that we talked about. If you haven't saved those constants
that you need, that could be an issue. You can also transform per batch
instead of for the entire dataset. So you could for example, normalize features by their
average within the batch. That only requires you to make
a pass over each batch and not the full data set, which when
you're working with terabytes of data. That can be a significant
advantage in terms of processing. And there's ways to
normalize per batch as well. So you can compute an average and
use that and normalization per batch and then do it again for the next batch there
will be differences batch to batch. Sometimes that's good in cases. So for example where you have
changes over time in a time series, that can be a good thing but
you need to consider that. So normalizing by the average per batch,
precompeting the average and using it during normalization, you can
use it for multiple batches for example. So this is different ways to try to think
about how to work with your data when you, especially when you have a large dataset. You need to think about optimizing
the instance level transformations as well because this is going to affect
both the training efficiency and you're serving efficiency. So you're going to have accelerators
that you need to consider. They may be sitting idle while
your CPU is doing transforms. That's something that you want to try to
avoid because accelerators tend to be expensive. So as a solution you can prefetch
your your transformations and use your accelerators more efficiently. So by prefetching you can
prefetch with your CPU feed your your accelerator your GPU or CPU and
try to paralyze that processing. So again this all gets down to cost and
how much it costs to train and and the time required as well to train
your model and how efficient it is. So to summarize the challenges we
have to balance the predictive performance of our model and
the requirements for it. Making folks passed transformations
on the training data is one thing. If we're going to do that then we need to
think about how that works when we serve our model as well and
save those constants. And we want to optimize the instance
level transformations for better training efficiency. So things like prefetching
can really help with that. So key points, inconsistent data
affects the accuracy of the results. So scalable data processing frameworks
allows to process large datasets and distributed inefficient ways. But we also need to think about
how that gets applied in serving.