Hello, and welcome to our
continuing saga of data. This week, we'll be talking about several data related topics, including metadata, which is
important for understanding our data over time and
are training runs. Will also talk about schemas which helped describe our Data. We'll look at some places
where we might want to store our data to make
it easier to work with. It's a lot to cover.
Let's get started. Welcome to Data journey
and Data Storage. Let's begin with the
Data journey and all it takes to understand
this important process. Understanding data provenance
requires understanding the Data journey throughout the life cycle of the
production pipeline. More specifically,
accounting for the evolution of data and models
throughout the process. ML metadata is a
versatile library to address these challenges, which helps with debugging
and reproducibility. Concretely, it allows us
to revisit and track data and model changes as they
occur during training. Trying to understand
data provenance begins with the Data journey. The journey starts
with raw features and labels from whatever
sources that we have. The Data describes a function
that maps the input in the training set to the labels that we're
trying to predict. During training, the model
learns the functional mapping from the input to the labels so as to be
as accurate as possible. The Data transforms and changes as part of
this training process. As the Data flows through
the process it transforms. Examples of this are, are changing data formats, applying feature engineering and training the model
to make predictions. There's a dual connection
between understanding these data transformations and interpreting the model's results. Therefore, it's important to track and document
these changes closely. Data artifacts are created as pipeline components execute. What exactly is an artifact? Each time a component
produces a result, it generates an artifact. This includes
basically everything that is produced by the pipeline, including the data in different
stages of transformation, often as a result of
feature engineering and the model itself and
things like the schema, and metrics and so forth. Basically, everything
that's produced, every result that is
produced as an artifact. The terms data provenance and data lineage are basically synonyms and they're
used interchangeably. Data provenance or
lineage is a sequence of the artifacts that are created as we move
through the pipeline. Those artifacts are
associated with a code and components
that we create. Tracking those sequences is
really key for debugging and understanding the training
process and comparing different training runs that
may happen months apart. Data provenance matters a
great deal and it helps us to understand the pipeline
and to perform debugging. Debugging and understanding
requires inspecting those artifacts at each point
in the training process, which can help us understand
how those artifacts were created and what the
results actually mean. Provenance will also allow
you to track back through a training run from any
point in the process. Also, provenance makes it possible to compare
training runs, and understand why they
produce different results. Under GDPR or the general
data protection regulation, organizations are
accountable for the origin, changes and location
of personal data. Personal data is
highly sensitive, so tracking the origins
and changes along the pipeline are
key for compliance. Data lineage is a great way for businesses and
organizations to quickly determined how the
Data has been used and which Transformations
were performed as the Data moved
through the pipeline. Data provenance is key to
interpreting model results. Model understanding
is related to this, but it's only part
of the picture. The model itself is an expression of the data
in the training set. In some sense, we can look at the model as a
transformation of the Data. Provenance also helps us
understand how the model evolves as it is trained
and perhaps optimized. Let's add an important
ingredient here, tracking different Data versions. Managing a data pipelines
is a big challenge as data evolves through the natural
life cycle of a project, over many different
training runs. A Machine learning when
it's done properly, should produce results that can be reproduced fairly
consistently. There will naturally
be some variance, but the results should be closed. Code version control is probably something
you're familiar with. GitHub is one of the most popular cloud-based
code repositories, and there are others as well. Environment versioning
is also important. Tools like Docker and
terraform help us create repeatable environments
and configuration. However, data versioning
is also important, and plays a significant
role for tracking provenance and lineage
data versioning. It's essentially
version control for data files so that you can trace changes over time and restore
previous versions early. But the tools are
somewhat different. For one thing, because of the size of files
that we deal with, which are typically
or can be any way, much larger than a code
file would ever be. Tools for data versioning are just starting to
become available. These include DVC and Git LFS. LFS for large files Storage.