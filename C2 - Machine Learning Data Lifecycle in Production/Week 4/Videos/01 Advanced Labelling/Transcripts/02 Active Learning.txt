Now let's explore
active learning. Active learning is a way to intelligently sample your data. The intelligent sampling
selects the unlabeled points that would bring the most
predictive value to your model. This is very helpful
in a variety of contexts: First of all,
limited data budget. It caused money to label data, especially when you're
using human experts to look at the data and
assign a label to it. For example, maybe in healthcare, active learning helps offset
this cost and burden. If you have an
imbalanced dataset, active learning is an
efficient way to select rare classes at the
training stage. If standard sampling
techniques don't help with improving accuracy
and other target metrics, active learning can find ways to achieve or help achieve
the desired accuracy. Active learning strategy
works by selecting labeled examples that will
best help the model learn. In a fully supervised setting, the training dataset consists of only the examples
that you've labeled. In a semi-supervised setting, you leverage those examples to perform some labeled propagation, so that's in addition
to active learning. This is the typical active
learning lifecycle: you start with a pool
of unlabeled data, and then active learning selects a few examples using
intelligent sampling, and we'll talk about what intelligent sampling
is in a second, and then you annotate
the data with human annotators or by
leveraging some other technique. This annotation or
labeling procedure generates up labeled
training dataset. Finally, you use
this labeled data to train a model and
make predictions. The cycle goes on, but this begs the question, how do we do
intelligent sampling? Margin sampling is one
widely used technique for doing intelligent sampling. In this example, the data
belong to two classes, additionally, they're
unlabeled data points. In this setting, the
simplest strategy is, we're just going to train a binary linear classifier model that just makes it simple. We're going to just use a
linear model for the example. We're going to train that
on the labeled data and that's going to give us
a decision boundary. Now, among the unlabeled data, the most uncertain
point is the one that is closest to the
decision boundary. With active learning,
you'll select that most uncertain point to be labeled next and
added to the dataset. Now, using this new
labeled data point as part of the dataset, you retrain the model to learn a new
classification boundary. Moving the boundary, the model learns a bit better to
separate those classes. Next, you find the most
uncertain data point, again, and repeat the process until the model doesn't improve. This plot shows model
accuracy as a function of the number of training examples
for sampling techniques. The red line shows the results of just selecting points
at random to label. The blue and green line
shows the performance of two margins sampling algorithms
using active learning. As you can see, the
margin sampling methods achieve a higher accuracy with fewer training examples than the random
sampling technique. But of course, eventually, as a higher percentage
of the unlabeled data is labeled with even
using random sampling, it will catch up to
margin sampling, as we see here, but it requires a lot more
data to be labeled. There are several common active learning
sampling techniques. With margin sampling,
as we just saw, you assign labels to the most uncertain points based on their distance from
the decision boundary. With cluster-based
sampling, you select a diverse set of points by using clustering methods over
your feature space. With query-by-committee, you
train several models and select the data points with the highest disagreement
among those models. Finally, region-based sampling is a relatively new algorithm. At a high level, this algorithm works by dividing
the input space into separate regions and running an active learning
algorithm in those regions.