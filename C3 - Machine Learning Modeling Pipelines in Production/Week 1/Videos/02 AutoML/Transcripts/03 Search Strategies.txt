But how does neural architecture
search decide which options in the search space to try next? It needs to have a search strategy,
let's discuss that next. Neural architecture search searches
through the search base for the architecture that produces
the best performance. A variety of different approaches
can be used for that search. These include grid search,
random search, Bayesian optimization, evolutionary algorithms and
reinforcement learning. In grid search you just search everything, that means you cover every option
we have in the search base. In random search, you select the next
option randomly within the search space. Both of these work reasonably
well in smaller search bases, but both also fail fairly quickly. When the search space grows beyond
a certain size, which is all too common. Bayesian optimization is
a little more sophisticated. It assumes that a specific probability
distribution, which is typically a Gaussian distribution is underlying
the performance of model architectures. So you use observations from
tested architectures to constrain the probability distribution and
guide the selection of the next option. This allows us to build up
an architecture stochastically based on the test results and
the constrained distribution. Neural architecture search can also
use an evolutionary method to search, here's how it works. First, an initial population of n
different model architecture is randomly generated,
the performance of each individual. In other words, each architecture
is evaluated as defined by the performance estimation strategy,
which we'll talk about next. Then the X highest performers are selected
as parents for a new generation. This new generation of
architectures might be copies of the respective parents with induced
random alterations or mutations. Or they might arise from
combinations of the parents, the performance of
the offspring is assessed. Again using the performance
estimation strategy. The list of possible mutations can
include operations like adding or removing a layer. Adding or removing a connection,
changing the size of a layer or changing another hyper parameter. Y architectures are selected to
be removed from the population. This might be the Y worst performers, the
Y oldest individuals in the population. Or a selection of individuals based
on a combination of these parameters. The offspring replaces
the removed architectures and the process is restarted
with this new population. In reinforcement learning agents
take actions in an environment, trying to maximize a reward. After each action, the state of the agent
and the environment is updated and a reward is issued based
on a performance metric. Then the range of possible
next actions is evaluated, the environment in this
case is our search space. And the reward function is our
performance estimation strategy. A neural network can also be
specified by a variable length string where the elements of the string
specify individual network layers. That enables us to use
a recurrent neural network or RNN to generate that string,
as we might do for an NLP model. The RNN that generates the string
is referred to as the controller, after training the network referred
to as the child network on real data. We can measure the accuracy
on the validation set, the accuracy determines the reinforcement
learning reward in this case. Based on the accuracy, we can compute the policy gradient
to update the controller RNN. In the next iteration, the controller
will have learned to give higher probabilities to architectures that result
in higher accuracies during training. This is how the controller will learn
to improve its search over time, for example, on the CIFAR-10 data set. This method starting from scratch, can design a new network architecture
that rivals the best human designed architecture in
terms of test set accuracy.