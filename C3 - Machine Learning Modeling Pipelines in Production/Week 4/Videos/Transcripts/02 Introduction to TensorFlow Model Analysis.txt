TensorFlow Model
Analysis, or TFMA is a versatile tool for doing deep analysis of your
model's performance. Let's discuss that now. Your top-level
metrics can easily hide problems with particular
parts of your data. For example, your model may not perform well for
particular customers, or particular products or
stores or days of the week, or different subsets
of your data that makes sense for your
domain or problem. Thinking about customers who are requesting a prediction
from your model. If you're model produces
a bad prediction, then that customer's experience
of your model is bad. Regardless of how well it
may do in top-level metrics. To enable developers to look at their performance
of their model at a deeper level Google created TensorFlow Model
Analysis or TFMA. TFMA is an open source, scalable framework for doing deep analysis of
model performance, including analyzing
performance on slices of data. TFMA is also a key part of a
machine-learning pipelines, such as a TFX pipeline, to perform deep
analysis before you deploy a newly trained
version of your model. TFMA has built-in capabilities
to check that your models meet certain quality
standards and to visualize evaluation metrics and inspect performance on
different slices of data. TFMA can be used by itself or as part of another
framework such as TFX. Let's take a look at the high
level architecture of TFMA. The TFMA pipeline is made
up of four main components. Read inputs, extractors,
evaluators, and write results. The first stage
read input is made up of a transform
that takes raw inputs such as CSV or TF records and so forth and it
converts it into a dictionary format which is understandable by the
next stage, extraction. Across all the stages, the output is kept in
this dictionary format, which is at the
datatype tfma.extracts. The next stage, extraction performs distributed
processing using Apache Beam. Input extractor and slice key extractor for slices
of the original dataset, which will be used by predict extractor to run
predictions on each slice. The results are sent
to the next stage, again, as a tfma.extracts
dictionary. The next stage evaluation also performs distributed
processing using Apache Beam. There are several
evaluators and you can create custom
evaluators as well. For example, the
metrics and plots evaluator extracts the
required fields from the data to evaluate the
performance of the model against the received predictions
from the previous stage. The final stage, write results, as the name suggests, writes the results to disk. TensorBoard and TFMA are used in different stages of
the development process. At a high level, TensorBoard is used to analyze the training process
itself while TFMA a is used to do deep analysis of the
finished trained model. TensorBoard is used to inspect
the training process of a single model often as you're monitoring your
progress during training. It can also be used to visualize the training progress
for more than one model with performance for
each plotted against their global training
steps as their training. After training is
finished TFMA allows developers to compare
different versions of their train models. While, TensorBoard
visualizes streaming metrics of multiple models over
global training sets TFMA visualizes the
metrics computed for a single model over multiple versions of the
exported saved model. Most model evaluation
results look at aggregate or a top-level metrics on
the entire training dataset. This aggregation often hides problems with model performance. For example, a model may have an acceptable AUC over
the entire EVAL dataset, but under performs
on specific slices. In general, a model with good
performance on average may exhibit failure
modes that are not apparent by looking at
an aggregate metric. Slicing metrics
allow you to analyze the performance of a model
on a more granular level. This functionality
enables developers to identify slices were
examples may be mislabeled or where the model
over or under predicts. For example, TFMA could be used to analyze whether
a model that predicts the generosity of a taxi tip works equally well
for riders that take the taxi during
the day versus at night by slicing the
data by the hour. In general, determining which
slices are important to analyze requires
domain knowledge about your data and application. TensorBoard confuse metrics on a mini-batch basis
during training. They're called Streaming
metrics and are our approximations based on
those observed mini-batches. TFMA uses Apache Beam to do a full pass over
the evil dataset. This not only allows for
accurate calculation of metrics, but also scales up to
massive evaluation dataset since Beam pipelines can be run using distributed
processing back ends. Note that TFMA may computes the same tensor flow metrics
that are computed by the TensorFlow EVAL
worker just more accurately by doing a full pass over the specified dataset. TFMA can also be configured
to compute additional metrics that were not identified or
not defined in the model. Furthermore, if
evaluation datasets are slice to compute metrics
for specific segments, each of those segments may only contain a small
number of examples. To accurately compute metrics, a deterministic full pass over those examples
is important.