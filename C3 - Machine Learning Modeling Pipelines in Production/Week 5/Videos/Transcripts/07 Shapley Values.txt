A key concept in measuring
feature importance and the contribution of each feature to
the models results is the Shapley value. Let's discuss that now. The Shapley value is a concept
from Cooperative Game Theory. It was named in honor of Lloyd Shapley
who introduced it in 1951 and won the Nobel Prize in Economics for
it in 2012. To each cooperative game it assigns
a unique distribution among the players of the total surplus value
generated by the coalition of all players. Here's how it works in game theory. Imagine that a group of
players cooperates and that results in an overall gain
because of their cooperation. Since some players may contribute more
than others or may possess different bargaining power, how should we
distribute the gains among the players? Or phrase differently, how important
is each player to the overall cooperation and what payoff can he or
she reasonably expect? The Shapley value provides one
possible answer to this question. So how do you apply this to ideas in ML? For machine learning and
interpret ability, the players are the features of the data
set, and we're using the Shapley value to determine how much each
feature contributes to the results. As you might expect
the prediction is the payout. Knowing how the features contribute will
help you understand what the important factors were in generating
the models results. Because it's not specific to any
particular type of model it can be used regardless of the model architecture. That was a quick overview of
the ideas behind Shapley value. Let's now focus on a concrete example, suppose you trained an ML model
to predict apartment prices. You need to explain why the model predicts
a €300,000 price for a certain apartment. What data do you have to work with? Well, in this example
the apartment is 50 square meters. It's located on the second floor,
it has a park nearby and cats are banned. You're not a cat, so you're okay. The average prediction for
all apartments is €310,000. Shapley values come from game theory, so let's clarify how to apply them to
machine learning interpret ability. The game is the prediction task for
a single instance of the data set. The gain is the actual prediction for this instance minus the average
prediction for all instances. The players are the feature
values of the instance that collaborate to produce the gain,
in the apartment example the feature values park equals nearby,
cat equals banned, area equals 50 square meter and
floor equals 2nd, work together to achieve
the prediction of €300,000. Our goal Is to explain the difference
between the actual prediction €300,000 and the average prediction of €310,000
which is the loss of €10,000. One possible explanation could be
the park nearby contributed €30,000, size of 50 square meter
contributed €10,000. Floor equals 2nd contributed zero. Cat equals banned contributed
negative €50,000. The contributions add
up to negative €10,000. The final prediction minus the average
predicted apartment price. You can think of that as the absolute
value €10,000 or you could also think about it as the percentage of
the average which is about 3.3%. This is one possible explanation,
but how did we get these numbers? Unlike perhaps any other method
of interpreting model results, Shapley values are based on
a solid theoretical foundation. Other methods make intuitive sense,
which is an important factor for interpret ability but don't have the same
rigorous theoretical foundation. This is one of the reasons that Shapley
was awarded the Nobel Prize for his work. The theory defines four properties
which must be satisfied, Efficiency, Symmetry,
Dummy and Additivity. One key advantage of Shapley values
that they're fairly distributed among the feature values of an instance. Some have argued that Shapley might be the
only method to deliver a full explanation, in situations where the law requires
explainability like the use right to explanations for example. Some feel that Shapley value might
be the only legally compliant method because it's based on a solid theory and
distributes the effects fairly. The Shapley value also allows
contrastive explanations. Instead of comparing a prediction to the
average prediction of the entire data set, you could compare it to a subset or
even to a single data point. This ability to contrast is something
that local models like Lime don't have. Like any method Shapley
has some disadvantages. Probably the most important is that
its computationally expensive, which in a large percentage of real world
cases means that it's only feasible to calculate at approximate solution. It can also be easily misinterpreted. The Shapley value is not the difference
of the predicted value after removing the feature from the model training,
it's the contribution of a feature value to the difference between the actual
prediction and the mean prediction. If you want to only explain
a few of your features, Shapley is probably the wrong method to
use, Shapley always uses all the features. Humans prefer selective explanations
such as those produced by Lime and other similar methods. So those might be the better choice for explanations that lay
persons have to deal with. Or another possible solution is to use
Shap which is based on the Shapley value but can provide explanations with only
a few features, we'll discuss Shap next. Unlike some other methods,
Shapley does not create a model. This means you can't use it to test
changes in the inputs, such as if I change 200 square meter apartment,
how does it change the prediction? And finally, like many other methods,
it does not work well when the features are correlated, but you already know
that you should have removed correlated features from your feature vector when
you were doing feature selection. So that's not a problem for you, right? Well, hopefully anyway but
something to be aware of.